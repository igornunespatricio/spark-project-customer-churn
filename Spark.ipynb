{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XQ6EdkxD2SEQ",
        "bdwRgwTy7LKT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up environment\n",
        "\n",
        "\n",
        "1.   Install Java\n",
        "2.   Install and unpack Spark and Hadoop Distributed File System (HDFS)\n",
        "3.   Define environment variables for Java and Spark\n",
        "4.   Install and initialize findspark\n",
        "5.   Check Spark installation\n",
        "\n"
      ],
      "metadata": {
        "id": "XQ6EdkxD2SEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Java installation\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "h3C6AHqF2WTy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading Spark and Hadoop Distributed File System (HDFS) files\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "\n",
        "# Unpacking files\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "TmUYHd7F3IX-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing os library\n",
        "import os\n",
        "\n",
        "# Defining Java's system variable\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# Defining Spark's system variable\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "LMZMRkk73N0t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary to run PySpark\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "rT6OI9tI5fwi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing installed finspark library\n",
        "import findspark\n",
        "\n",
        "# Initialize findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "r3WwHA115pGh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing package to initialize Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# iniciando o spark context\n",
        "sc = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "\n",
        "# Verificando se a sess√£o foi criada\n",
        "sc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fQ2a2f1v5xiJ",
        "outputId": "d20123a7-8721-433c-d18e-623334eb4d7f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x78d85019add0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://5f428aa93f26:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If everything is right, the output of the last cell should per below:\n",
        "\n",
        "![successful spark instalation.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "u6XsYCJc6fvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading Kaggle Dataset\n",
        "\n",
        "In this case, a customer-churn dataset found [here](https://www.kaggle.com/datasets/muhammadshahidazeem/customer-churn-dataset) will be used. More info on the dataset in this link.\n",
        "\n",
        "1. Install Kaggle package\n",
        "2. Upload kaggle.json file with credentials (follow instructions [here](https://www.kaggle.com/discussions/general/74235))\n",
        "3. Create kaggle directory\n",
        "4. Upload kaggle.json file\n",
        "5. Change permition of kaggle json file\n",
        "6. Copy API command from the Kaggle dataset you want to use by clicking in $\\ \\vdots$  \n",
        " and then \"Copy API Command\". In this case, [this](https://www.kaggle.com/datasets/muhammadshahidazeem/customer-churn-dataset) dataset is being used\n",
        "7. Paste the copied command above in a new code cell and run\n",
        "8. After waiting the download to finish, under the /content diretory you should have the zipped file with the dataset in the /content directory\n",
        "9. Unzip the data into a directory of choice. In this case, \"customer-churn\""
      ],
      "metadata": {
        "id": "bdwRgwTy7LKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Kaggle package\n",
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "Wf8PqZ0u-npT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload your Kaggles API secret KEY in file kaggle.json (link: https://www.kaggle.com/discussions/general/74235)\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "Ba9Js3wLAyaH",
        "outputId": "62bd4795-a56e-4eeb-8fb9-2cebcb9d23b2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cc788b49-2995-4d34-9bec-452185fc1041\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cc788b49-2995-4d34-9bec-452185fc1041\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"igornunespatricio\",\"key\":\"7f5d35c1c5bd7334abed0a8e12172c4c\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create kaggle directory\n",
        "!mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "68kh-oeKBKav"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy kaggle json file to the created directory\n",
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "hgJ1OyaZDPCd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change permission of the file\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "hxb81dm9Dips"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if everything is correct\n",
        "!kaggle datasets list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zLhw1e1DwgS",
        "outputId": "4629f736-8714-48b4-fdd5-7e1b3f32361c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                         title                                                size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "----------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "nelgiriyewithana/countries-of-the-world-2023                Global Country Information Dataset 2023              23KB  2023-07-08 20:37:33           7796        290  1.0              \n",
            "alphiree/cardiovascular-diseases-risk-prediction-dataset    Cardiovascular Diseases Risk Prediction Dataset       5MB  2023-07-03 12:12:19           7651        274  1.0              \n",
            "arnavsmayan/netflix-userbase-dataset                        Netflix Userbase Dataset                             25KB  2023-07-04 07:38:41           9281        166  1.0              \n",
            "joebeachcapital/top-10000-spotify-songs-1960-now            Top 10000 Songs on Spotify 1960-Now                   2MB  2023-07-26 00:54:14           1063         44  1.0              \n",
            "atharvaarya25/e-commerce-analysis-uk                        E-Commerce Analysis - UK                              7MB  2023-07-26 05:25:41            901         31  1.0              \n",
            "kapturovalexander/bank-credit-scoring                       Bank credit scoring                                  65KB  2023-08-04 13:49:41            479         25  1.0              \n",
            "crxxom/spotify-popular-east-asian-artists-and-tracks        Spotify Popular East Asian Artists and Tracks         1MB  2023-07-22 17:39:23           1034         36  1.0              \n",
            "deependraverma13/diabetes-healthcare-comprehensive-dataset  Diabetes Healthcare: Comprehensive Dataset-AI         9KB  2023-07-23 04:24:59            718         28  1.0              \n",
            "gianinamariapetrascu/japan-life-expectancy                  üëµüèª Japan life expectancy                              1MB  2023-07-26 17:56:28            509         34  1.0              \n",
            "harishkumardatalab/housing-price-prediction                 Housing Price Prediction                              5KB  2023-07-07 04:34:24           3033         65  1.0              \n",
            "sudheerp2147234/salary-dataset-based-on-country-and-race    Salary dataset based on country and race             49KB  2023-07-06 09:10:21           2074         43  1.0              \n",
            "byomokeshsenapati/spotify-song-attributes                   Spotify Song Attributes                             883KB  2023-07-09 16:00:20           1816         50  1.0              \n",
            "gyanprakashkushwaha/realme-mobiles-dataset                  Realme Mobiles Dataset                               56KB  2023-07-26 11:06:40            401         27  1.0              \n",
            "architsharma01/loan-approval-prediction-dataset             Loan-Approval-Prediction-Dataset                     81KB  2023-07-16 16:31:20           1352         34  1.0              \n",
            "bhanupratapbiswas/zomato                                    ZomatoüåÆüçùü•ß                                             5MB  2023-07-20 08:47:17            899         35  1.0              \n",
            "bhanupratapbiswas/superstore-sales                          Superstore Sales üõíüè∑Ô∏èüõçÔ∏èüì¶üè™                            478KB  2023-07-20 09:11:47           1362         38  1.0              \n",
            "drahulsingh/list-of-shopping-malls-in-indiacsv              List-of-shopping-malls-in-India.csv                   8KB  2023-07-21 17:37:15            677         36  1.0              \n",
            "gayu14/tv-and-movie-metadata-with-genres-and-ratings-imbd   TV & Movie Metadata with Genres and Ratings (2023)   16MB  2023-08-01 09:37:02            454         22  0.9411765        \n",
            "bhanupratapbiswas/olympic-data                              Olympic Data ü•á ‚õ≥ü•ÖüèãÔ∏è‚Äç‚ôÄÔ∏èüö¥‚Äç‚ôÇÔ∏è                            1MB  2023-07-20 09:30:19           1623         46  1.0              \n",
            "mauryansshivam/paytm-revenue-users-transactions             Paytm Revenue, Users, Transactions                   968B  2023-07-10 10:13:35            985         28  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the Dataset from Kaggle\n",
        "!kaggle datasets download -d muhammadshahidazeem/customer-churn-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ebi7alJ-sru",
        "outputId": "449c8d1a-d852-4655-9c4e-4a824500c442"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading customer-churn-dataset.zip to /content\n",
            "\r  0% 0.00/6.66M [00:00<?, ?B/s]\r 75% 5.00M/6.66M [00:00<00:00, 38.5MB/s]\n",
            "\r100% 6.66M/6.66M [00:00<00:00, 49.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzipping content to another folder\n",
        "!unzip /content/customer-churn-dataset.zip -d customer-churn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ps8yicsWHzmQ",
        "outputId": "1b663a1b-92bb-461c-fced-08bde2da3677"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/customer-churn-dataset.zip\n",
            "  inflating: customer-churn/customer_churn_dataset-testing-master.csv  \n",
            "  inflating: customer-churn/customer_churn_dataset-training-master.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Spark"
      ],
      "metadata": {
        "id": "UK6kCWK-Jid0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Spark session\n",
        "FILEPATH = \"/content/customer-churn/customer_churn_dataset-training-master.csv\"\n",
        "sp = SparkSession.builder.master(\"local[1]\")\\\n",
        "          .appName(\"Customer-Churn\")\\\n",
        "          .getOrCreate()\n",
        "# Read csv file (if another file  type, search PySpark documentation)\n",
        "df = sp.read.options(header=True).csv(FILEPATH)"
      ],
      "metadata": {
        "id": "bUpG9nAaJlln"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show 10 first rows of dataframe\n",
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUnG0vc8Ny7g",
        "outputId": "dd020dd5-c32d-4dee-c78c-03949dd006a7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---+------+------+---------------+-------------+-------------+-----------------+---------------+-----------+----------------+-----+\n",
            "|CustomerID|Age|Gender|Tenure|Usage Frequency|Support Calls|Payment Delay|Subscription Type|Contract Length|Total Spend|Last Interaction|Churn|\n",
            "+----------+---+------+------+---------------+-------------+-------------+-----------------+---------------+-----------+----------------+-----+\n",
            "|         2| 30|Female|    39|             14|            5|           18|         Standard|         Annual|        932|              17|    1|\n",
            "|         3| 65|Female|    49|              1|           10|            8|            Basic|        Monthly|        557|               6|    1|\n",
            "|         4| 55|Female|    14|              4|            6|           18|            Basic|      Quarterly|        185|               3|    1|\n",
            "|         5| 58|  Male|    38|             21|            7|            7|         Standard|        Monthly|        396|              29|    1|\n",
            "|         6| 23|  Male|    32|             20|            5|            8|            Basic|        Monthly|        617|              20|    1|\n",
            "|         8| 51|  Male|    33|             25|            9|           26|          Premium|         Annual|        129|               8|    1|\n",
            "|         9| 58|Female|    49|             12|            3|           16|         Standard|      Quarterly|        821|              24|    1|\n",
            "|        10| 55|Female|    37|              8|            4|           15|          Premium|         Annual|        445|              30|    1|\n",
            "|        11| 39|  Male|    12|              5|            7|            4|         Standard|      Quarterly|        969|              13|    1|\n",
            "|        12| 64|Female|     3|             25|            2|           11|         Standard|      Quarterly|        415|              29|    1|\n",
            "+----------+---+------+------+---------------+-------------+-------------+-----------------+---------------+-----------+----------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(how=\"any\")"
      ],
      "metadata": {
        "id": "juHt2vDgeut7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Showing dataset Schema\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kguq5x4cO9ta",
        "outputId": "cee9f7e7-b73c-425f-fb2d-764381b03d9c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CustomerID: string (nullable = true)\n",
            " |-- Age: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Tenure: string (nullable = true)\n",
            " |-- Usage Frequency: string (nullable = true)\n",
            " |-- Support Calls: string (nullable = true)\n",
            " |-- Payment Delay: string (nullable = true)\n",
            " |-- Subscription Type: string (nullable = true)\n",
            " |-- Contract Length: string (nullable = true)\n",
            " |-- Total Spend: string (nullable = true)\n",
            " |-- Last Interaction: string (nullable = true)\n",
            " |-- Churn: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count rows and columns\n",
        "print(f'Number of rows: {df.count()}')\n",
        "print(f'Number of columns: {len(df.columns)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uPJFaupcxWD",
        "outputId": "f8db224a-c802-4b6b-9904-62fede882033"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 440832\n",
            "Number of columns: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicates by Customer ID\n",
        "df = df.dropDuplicates(['CustomerID'])"
      ],
      "metadata": {
        "id": "KWvGndLGe9hZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing column type\n",
        "from pyspark.sql.types import (\n",
        "    StringType, BooleanType, IntegerType, FloatType, DoubleType\n",
        ")\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df = df.withColumn(\"Age\", col(\"Age\").cast(IntegerType()))\\\n",
        ".withColumn(\"Tenure\", col(\"Tenure\").cast(IntegerType()))\\\n",
        ".withColumn(\"Usage Frequency\", col(\"Usage Frequency\").cast(IntegerType()))\\\n",
        ".withColumn(\"Support Calls\", col(\"Support Calls\").cast(IntegerType()))\\\n",
        ".withColumn(\"Payment Delay\", col(\"Payment Delay\").cast(IntegerType()))\\\n",
        ".withColumn(\"Total Spend\", col(\"Total Spend\").cast(FloatType()))\\\n",
        ".withColumn(\"Last Interaction\", col(\"Last Interaction\").cast(IntegerType()))\\\n",
        ".withColumn(\"Churn\", col(\"Churn\").cast(IntegerType()))\\\n",
        ".drop(col(\"CustomerID\"))\n",
        "\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "DUiHvbXKQXXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6cf1581-852e-499a-96a5-791ffbdaa174"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Tenure: integer (nullable = true)\n",
            " |-- Usage Frequency: integer (nullable = true)\n",
            " |-- Support Calls: integer (nullable = true)\n",
            " |-- Payment Delay: integer (nullable = true)\n",
            " |-- Subscription Type: string (nullable = true)\n",
            " |-- Contract Length: string (nullable = true)\n",
            " |-- Total Spend: float (nullable = true)\n",
            " |-- Last Interaction: integer (nullable = true)\n",
            " |-- Churn: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing column name to replace space with _\n",
        "for column in df.columns:\n",
        "    df = df.withColumnRenamed(column, column if ' ' not in column else column.replace(' ','_'))\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saJ2XrR7CDPI",
        "outputId": "257192fa-8dc1-4ce6-e19d-be2a201c05f7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Tenure: integer (nullable = true)\n",
            " |-- Usage_Frequency: integer (nullable = true)\n",
            " |-- Support_Calls: integer (nullable = true)\n",
            " |-- Payment_Delay: integer (nullable = true)\n",
            " |-- Subscription_Type: string (nullable = true)\n",
            " |-- Contract_Length: string (nullable = true)\n",
            " |-- Total_Spend: float (nullable = true)\n",
            " |-- Last_Interaction: integer (nullable = true)\n",
            " |-- Churn: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "# Number of Churns and not Churns\n",
        "df.groupBy(\"Churn\") \\\n",
        "    .agg(\n",
        "        F.count(\"Churn\").alias(\"num_people\"), \\\n",
        "    )\\\n",
        "    .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taA8MamZ2hKW",
        "outputId": "cfc29cb8-3def-4b5d-a9fb-705707f80540"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+\n",
            "|Churn|num_people|\n",
            "+-----+----------+\n",
            "|    1|    249999|\n",
            "|    0|    190833|\n",
            "+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import struct\n",
        "def groupby_and_describe(df, group_col, stat_col):\n",
        "    \"\"\"\n",
        "    df has to be a pyspark dataframe\n",
        "    Compute statistics for the column in stat_col after grouping\n",
        "    by group_col.\n",
        "    if stat_col is IntegerType or FloatType, calculates quantiles, median, max,...\n",
        "    if stat_col is StringType, count occurence by each category\n",
        "    Take the resulting datafarame and nest the statistics columns.\n",
        "    \"\"\"\n",
        "    assert group_col != stat_col, f\"group_col and stat_col can't be the same, they are {group_col} and {stat_col}, respectively.\"\n",
        "\n",
        "    grouped = df.groupby(group_col)\n",
        "    column = df.schema[stat_col]\n",
        "    # if column is IntegerType or FloatType\n",
        "    if (isinstance(column.dataType, IntegerType) or isinstance(column.dataType, FloatType)) and (column.name != group_col):\n",
        "        # group and create statistics\n",
        "        output = grouped.agg(\n",
        "            F.mean(stat_col).alias(\"mean\"),\n",
        "            F.stddev(stat_col).alias(\"std\"),\n",
        "            F.min(stat_col).alias(\"min\"),\n",
        "            F.percentile_approx(stat_col, 0.25).alias(\"25%\"),\n",
        "            F.percentile_approx(stat_col, 0.50).alias(\"50%\"),\n",
        "            F.percentile_approx(stat_col, 0.75).alias(\"75%\"),\n",
        "            F.max(stat_col).alias(\"max\"),\n",
        "        )\n",
        "        # structure the dataframe\n",
        "        output = output.select(\n",
        "            group_col,\n",
        "            struct(\"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\")\\\n",
        "            .alias(stat_col)\n",
        "        )\n",
        "        return output\n",
        "    # if the column is StringType\n",
        "    elif isinstance(column.dataType, StringType) and (column.name != group_col):\n",
        "        # group by group_col and pivot the stat_col and count occurences\n",
        "        output = df.groupby(group_col).pivot(stat_col).count()\n",
        "        # change columns from long to IntegerType\n",
        "        cols2change = [col for col in output.columns if col != group_col]\n",
        "        for column in cols2change:\n",
        "            output = output.withColumn(column, col(column).cast(IntegerType()))\n",
        "        # Structure the dataframe\n",
        "        output = output.select(\n",
        "            group_col,\n",
        "            struct(cols2change)\\\n",
        "            .alias(stat_col)\n",
        "        )\n",
        "        return output\n",
        "\n",
        "# Join statistics for each column by Churn value keeping nested columns\n",
        "joined = None\n",
        "aggregate_cols = [col for col in df.columns if col != \"Churn\"]\n",
        "for column in aggregate_cols:\n",
        "    print(f'Variable: {column}')\n",
        "    grouped_temp = groupby_and_describe(\n",
        "        df=df,\n",
        "        group_col=\"Churn\",\n",
        "        stat_col=column\n",
        "        )\n",
        "    if not joined:\n",
        "        joined = grouped_temp\n",
        "    else:\n",
        "        joined = joined.join(grouped_temp, on=[\"Churn\"])\n",
        "\n",
        "joined.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYotM71cqxD-",
        "outputId": "d0841708-29a3-4943-e869-c226d36f9a2d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variable: Age\n",
            "Variable: Gender\n",
            "Variable: Tenure\n",
            "Variable: Usage_Frequency\n",
            "Variable: Support_Calls\n",
            "Variable: Payment_Delay\n",
            "Variable: Subscription_Type\n",
            "Variable: Contract_Length\n",
            "Variable: Total_Spend\n",
            "Variable: Last_Interaction\n",
            "root\n",
            " |-- Churn: integer (nullable = true)\n",
            " |-- Age: struct (nullable = false)\n",
            " |    |-- mean: double (nullable = true)\n",
            " |    |-- std: double (nullable = true)\n",
            " |    |-- min: integer (nullable = true)\n",
            " |    |-- 25%: integer (nullable = true)\n",
            " |    |-- 50%: integer (nullable = true)\n",
            " |    |-- 75%: integer (nullable = true)\n",
            " |    |-- max: integer (nullable = true)\n",
            " |-- Gender: struct (nullable = false)\n",
            " |    |-- Female: integer (nullable = true)\n",
            " |    |-- Male: integer (nullable = true)\n",
            " |-- Tenure: struct (nullable = false)\n",
            " |    |-- mean: double (nullable = true)\n",
            " |    |-- std: double (nullable = true)\n",
            " |    |-- min: integer (nullable = true)\n",
            " |    |-- 25%: integer (nullable = true)\n",
            " |    |-- 50%: integer (nullable = true)\n",
            " |    |-- 75%: integer (nullable = true)\n",
            " |    |-- max: integer (nullable = true)\n",
            " |-- Usage_Frequency: struct (nullable = false)\n",
            " |    |-- mean: double (nullable = true)\n",
            " |    |-- std: double (nullable = true)\n",
            " |    |-- min: integer (nullable = true)\n",
            " |    |-- 25%: integer (nullable = true)\n",
            " |    |-- 50%: integer (nullable = true)\n",
            " |    |-- 75%: integer (nullable = true)\n",
            " |    |-- max: integer (nullable = true)\n",
            " |-- Support_Calls: struct (nullable = false)\n",
            " |    |-- mean: double (nullable = true)\n",
            " |    |-- std: double (nullable = true)\n",
            " |    |-- min: integer (nullable = true)\n",
            " |    |-- 25%: integer (nullable = true)\n",
            " |    |-- 50%: integer (nullable = true)\n",
            " |    |-- 75%: integer (nullable = true)\n",
            " |    |-- max: integer (nullable = true)\n",
            " |-- Payment_Delay: struct (nullable = false)\n",
            " |    |-- mean: double (nullable = true)\n",
            " |    |-- std: double (nullable = true)\n",
            " |    |-- min: integer (nullable = true)\n",
            " |    |-- 25%: integer (nullable = true)\n",
            " |    |-- 50%: integer (nullable = true)\n",
            " |    |-- 75%: integer (nullable = true)\n",
            " |    |-- max: integer (nullable = true)\n",
            " |-- Subscription_Type: struct (nullable = false)\n",
            " |    |-- Basic: integer (nullable = true)\n",
            " |    |-- Premium: integer (nullable = true)\n",
            " |    |-- Standard: integer (nullable = true)\n",
            " |-- Contract_Length: struct (nullable = false)\n",
            " |    |-- Annual: integer (nullable = true)\n",
            " |    |-- Monthly: integer (nullable = true)\n",
            " |    |-- Quarterly: integer (nullable = true)\n",
            " |-- Total_Spend: struct (nullable = false)\n",
            " |    |-- mean: double (nullable = true)\n",
            " |    |-- std: double (nullable = true)\n",
            " |    |-- min: float (nullable = true)\n",
            " |    |-- 25%: float (nullable = true)\n",
            " |    |-- 50%: float (nullable = true)\n",
            " |    |-- 75%: float (nullable = true)\n",
            " |    |-- max: float (nullable = true)\n",
            " |-- Last_Interaction: struct (nullable = false)\n",
            " |    |-- mean: double (nullable = true)\n",
            " |    |-- std: double (nullable = true)\n",
            " |    |-- min: integer (nullable = true)\n",
            " |    |-- 25%: integer (nullable = true)\n",
            " |    |-- 50%: integer (nullable = true)\n",
            " |    |-- 75%: integer (nullable = true)\n",
            " |    |-- max: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "boxplots = {}\n",
        "\n",
        "for column in df.schema:\n",
        "    print(column.name)\n",
        "    temp = df.select(\"Churn\", column.name)\n",
        "    if (isinstance(column.dataType, IntegerType) or isinstance(column.dataType, FloatType)) and (column.name != \"Churn\"):\n",
        "        fig = px.box(\n",
        "            temp.toPandas(),\n",
        "            x=\"Churn\", y=column.name, color=\"Churn\",\n",
        "            title=f\"{column.name} x Churn\"\n",
        "        )\\\n",
        "        .update_layout(\n",
        "            yaxis_title=None, xaxis_title=None,\n",
        "            title_x=0.5, title_font_size=30\n",
        "        )\n",
        "        boxplots[column.name] = fig\n",
        "    elif (isinstance(column.dataType, StringType) and (column.name != \"Churn\")):\n",
        "        # create bar chart with plotly\n",
        "        grouped_test = df.select(\"Churn\", column.name).groupby(\"Churn\").pivot(column.name).count().withColumn(\"Churn\", F.col(\"Churn\").cast(\"string\"))\n",
        "        y_axis = [item for item in grouped_test.columns if item != \"Churn\"]\n",
        "        fig = px.histogram(\n",
        "            grouped_test.toPandas(),\n",
        "            x=\"Churn\",\n",
        "            y=y_axis,\n",
        "            barnorm=\"percent\",\n",
        "            text_auto='.2f'\n",
        "        ).update_layout(\n",
        "            yaxis_title=None, xaxis_title=None, title=f\"Churn x {column.name} (%)\",\n",
        "            title_x=0.5, title_font_size=30\n",
        "        )\n",
        "        boxplots[column.name] = fig\n",
        "\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "if not os.path.isdir(\"charts\"):\n",
        "    os.mkdir(\"charts\")\n",
        "\n",
        "for key, chart in boxplots.items():\n",
        "    chart.write_html(f\"charts/boxplot_{key}_x_Churn.html\")\n",
        "\n",
        "# for downloading files one by one\n",
        "# [files.download(\"charts/\" + item) for item in os.listdir(\"charts\")]\n",
        "\n",
        "# for creating a zip file with the charts and downloading it\n",
        "!zip -r charts.zip charts/\n",
        "files.download('charts.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "3k3mNoloi8Fq",
        "outputId": "5cf394fd-f2be-4019-b7e6-3adca75e1379"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Age\n",
            "Gender\n",
            "Tenure\n",
            "Usage_Frequency\n",
            "Support_Calls\n",
            "Payment_Delay\n",
            "Subscription_Type\n",
            "Contract_Length\n",
            "Total_Spend\n",
            "Last_Interaction\n",
            "Churn\n",
            "  adding: charts/ (stored 0%)\n",
            "  adding: charts/boxplot_Gender_x_Churn.html (deflated 70%)\n",
            "  adding: charts/boxplot_Contract_Length_x_Churn.html (deflated 70%)\n",
            "  adding: charts/boxplot_Payment_Delay_x_Churn.html (deflated 74%)\n",
            "  adding: charts/boxplot_Total_Spend_x_Churn.html (deflated 76%)\n",
            "  adding: charts/boxplot_Tenure_x_Churn.html (deflated 74%)\n",
            "  adding: charts/boxplot_Subscription_Type_x_Churn.html (deflated 70%)\n",
            "  adding: charts/boxplot_Usage_Frequency_x_Churn.html (deflated 74%)\n",
            "  adding: charts/boxplot_Last_Interaction_x_Churn.html (deflated 74%)\n",
            "  adding: charts/boxplot_Support_Calls_x_Churn.html (deflated 76%)\n",
            "  adding: charts/boxplot_Age_x_Churn.html (deflated 74%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7e3ff677-51bc-4839-aeba-d498aff3be9a\", \"charts.zip\", 14272418)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning"
      ],
      "metadata": {
        "id": "ei4QRDE23Wx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "NHEV5rI13z62",
        "outputId": "40b65f01-4a45-4d77-fab9-ab70fd2c36df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Tenure: integer (nullable = true)\n",
            " |-- Usage_Frequency: integer (nullable = true)\n",
            " |-- Support_Calls: integer (nullable = true)\n",
            " |-- Payment_Delay: integer (nullable = true)\n",
            " |-- Subscription_Type: string (nullable = true)\n",
            " |-- Contract_Length: string (nullable = true)\n",
            " |-- Total_Spend: float (nullable = true)\n",
            " |-- Last_Interaction: integer (nullable = true)\n",
            " |-- Churn: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_churns = df.rdd.map(lambda x: x[\"Churn\"]).sum()\n",
        "total = df.rdd.map(lambda x: x[\"Churn\"]).count()\n",
        "print(f'Churns: {num_churns}\\nTotal Churns: {total}\\nProportion of Churns: {num_churns/total:.2%}')"
      ],
      "metadata": {
        "id": "086zpZk_73fn",
        "outputId": "fbac95bb-6493-4ff2-c345-440fae8a22aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Churns: 249999\n",
            "Total Churns: 440832\n",
            "Proportion of Churns: 56.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A \"model\" predicting only churns would predict 56.71% correct answers (which in this case would also be the accuracy)."
      ],
      "metadata": {
        "id": "ddiJpK8pIOow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dividing training and validation"
      ],
      "metadata": {
        "id": "oEYi8Y_yVctx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividing 5% as validation and 30% as training, drop the rest for now\n",
        "train, val = df.randomSplit([0.3, 0.05], seed=0)\n",
        "print(f\"Training examples: {train.count()}\\nVal examples: {val.count()}\")"
      ],
      "metadata": {
        "id": "eTOBMAYJS3qE",
        "outputId": "88023e93-4f02-448c-a6b6-23a9af2a9bda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training examples: 377979\n",
            "Val examples: 62853.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1:\n",
        "\n",
        "- Base Model\n",
        "- Not using StringType columns\n",
        "- No Feature Engineering\n",
        "- No normalization\n",
        "- Using only IntegerType and FloatType columns\n",
        "- Nothing else was done"
      ],
      "metadata": {
        "id": "sk_qv11oKINo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from pyspark.ml import Transformer\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from xgboost.spark import SparkXGBClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark import keyword_only\n",
        "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
        "\n",
        "class KeepIntFloatCols(Transformer, DefaultParamsWritable, DefaultParamsReadable):\n",
        "    \"\"\"\n",
        "    Transformer class that only keeps IntegerType and FloatType columns.\n",
        "    Also maintains the label column which the user should pass in the parameter\n",
        "    called labelCol.\n",
        "    \"\"\"\n",
        "    def __init__(self, labelCol: string):\n",
        "        super(KeepIntFloatCols, self).__init__()\n",
        "        self.labelCol = labelCol\n",
        "\n",
        "    def _transform(self, df: DataFrame) -> DataFrame:\n",
        "        cols = [column.name for column in df.schema if (isinstance(column.dataType, IntegerType) or isinstance(column.dataType, FloatType) or column.name == self.labelCol)]\n",
        "        return df.select(cols)\n",
        "\n",
        "class CustomVectorAssembler(VectorAssembler):\n",
        "    \"\"\"\n",
        "    The only difference from VectorAssembler class is that:\n",
        "    1) Removes the label column (labelCol parameter) from the assembler\n",
        "    2) Set all columns from df as InputCols (if as a stage in pipeline, uses the previous df)\n",
        "    \"\"\"\n",
        "    @keyword_only\n",
        "    def __init__(self, outputCol: string = 'features', labelCol: string = 'label'):\n",
        "        super(CustomVectorAssembler, self).__init__()\n",
        "        self.transformer = VectorAssembler(outputCol=outputCol)\n",
        "        kwargs = self._input_kwargs\n",
        "        self.labelCol = labelCol\n",
        "        self.setParams(**kwargs)\n",
        "\n",
        "    @keyword_only\n",
        "    def setParams(self, outputCol: string ='features', labelCol: string = 'label'):\n",
        "        kwargs = self._input_kwargs\n",
        "        return self._set(**kwargs)\n",
        "\n",
        "    def _transform(self, df) -> DataFrame:\n",
        "        ready = [col for col in df.columns if col != self.labelCol]\n",
        "        self.setInputCols(ready)\n",
        "        self.transformer.setInputCols(ready)\n",
        "        df = self.transformer.transform(df)\n",
        "        return df\n",
        "\n",
        "# Creating tranformer object for keeping only IntegerType and FloatType columns (include also labelCol)\n",
        "remove_cols = KeepIntFloatCols(labelCol=\"Churn\")\n",
        "\n",
        "# Identifies categorical features and indexes then if categories are at a 4 maximum\n",
        "# vectorIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"features\", maxCategories=4)\n",
        "\n",
        "# Creating transformer object for vector assemble (with input cols from the previous stage/df in pipeline)\n",
        "custom_vector_assembler = CustomVectorAssembler(outputCol='features', labelCol='Churn')\n",
        "\n",
        "# Creating the XGBoost Classifier: input col is 'features'by default\n",
        "xgb_classifier = SparkXGBClassifier(num_workers=2, label_col=\"Churn\", missing=0.0)\n",
        "\n",
        "# Object with grid of parameters to try\n",
        "paramGrid = ParamGridBuilder()\\\n",
        "  .addGrid(xgb_classifier.max_depth, [2, 5])\\\n",
        "  .addGrid(xgb_classifier.n_estimators, [10, 100])\\\n",
        "  .build()\n",
        "\n",
        "# Define object for evaluation metric\n",
        "evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=xgb_classifier.getLabelCol(),\n",
        "    rawPredictionCol=xgb_classifier.getPredictionCol()\n",
        ")\n",
        "\n",
        "# Define object for crossvalidation (model tuning).\n",
        "cv = CrossValidator(estimator=xgb_classifier, evaluator=evaluator, estimatorParamMaps=paramGrid)"
      ],
      "metadata": {
        "id": "4LsrcLgH3oyF"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the pipeline\n",
        "pipeline = Pipeline(stages=[remove_cols, custom_vector_assembler, cv])"
      ],
      "metadata": {
        "id": "kIAveRZsVSWw"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "pipelineModel = pipeline.fit(train)"
      ],
      "metadata": {
        "id": "SL9r0hR7ZSkK",
        "outputId": "8f389737-cf7a-411a-dfce-fa3951d03a2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py:782: UserWarning:\n",
            "\n",
            "Loading a native XGBoost model with Scikit-Learn interface.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions on validation set\n",
        "predictions = pipelineModel.transform(val)"
      ],
      "metadata": {
        "id": "i7FEOwSVgWb3"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.select(\"Churn\", \"prediction\", \"probability\", \"rawPrediction\", *predictions.columns[:-5]).show()"
      ],
      "metadata": {
        "id": "OXaxfDctg8cE",
        "outputId": "498a64bd-765c-4e58-c452-e6f588f8fd4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+--------------------+--------------------+---+------+---------------+-------------+-------------+-----------+----------------+\n",
            "|Churn|prediction|         probability|       rawPrediction|Age|Tenure|Usage_Frequency|Support_Calls|Payment_Delay|Total_Spend|Last_Interaction|\n",
            "+-----+----------+--------------------+--------------------+---+------+---------------+-------------+-------------+-----------+----------------+\n",
            "|    1|       1.0|           [0.0,1.0]|[-19.365781784057...| 18|    55|              2|            6|           18|      334.0|               6|\n",
            "|    0|       0.0|[0.85511559247970...|[1.77530050277709...| 18|    13|             29|            1|            4|      832.0|              21|\n",
            "|    0|       0.0|[0.98779720067977...|[4.39381408691406...| 18|    15|             15|            1|            7|     977.05|              14|\n",
            "|    1|       1.0|[5.44786453247070...|[-9.8184785842895...| 18|    30|             16|            1|           30|     525.68|              19|\n",
            "|    0|       0.0|[0.97681260108947...|[3.74068474769592...| 18|    37|             18|            0|           18|     720.21|               6|\n",
            "|    1|       1.0|[3.93390655517578...|[-12.448019027709...| 19|     3|             30|            4|           15|      636.0|               7|\n",
            "|    0|       0.0|[0.98021757602691...|[3.90298104286193...| 19|     8|             27|            1|           20|     538.84|              13|\n",
            "|    0|       0.0|[0.96716260910034...|[3.38279843330383...| 19|    39|              4|            0|            8|     693.38|               2|\n",
            "|    0|       0.0|[0.97431904077529...|[3.63598847389221...| 19|    48|              8|            1|           17|     891.44|               5|\n",
            "|    1|       1.0|           [0.0,1.0]|[-21.544013977050...| 19|    19|              3|            6|           15|      331.0|               2|\n",
            "|    1|       1.0|           [0.0,1.0]|[-18.355636596679...| 19|    24|             24|            9|           14|      475.0|              21|\n",
            "|    0|       0.0|[0.97602075338363...|[3.70629596710205...| 19|    57|             18|            1|           19|     963.91|               6|\n",
            "|    1|       1.0|           [0.0,1.0]|[-18.816310882568...| 19|    59|              6|           10|           25|      520.0|              15|\n",
            "|    1|       1.0|           [0.0,1.0]|[-18.681175231933...| 19|    59|             28|            8|           24|      880.0|              12|\n",
            "|    1|       1.0|[1.43051147460937...|[-13.431241989135...| 20|    54|              7|            5|            1|      492.0|              19|\n",
            "|    0|       0.0|[0.92969906330108...|[2.58207607269287...| 20|     3|             16|            0|            8|     504.08|              12|\n",
            "|    0|       0.0|[0.89047908782958...|[2.09564423561096...| 20|     5|              5|            0|            2|     971.56|              20|\n",
            "|    0|       0.0|[0.92662149667739...|[2.53591442108154...| 20|    31|              1|            1|           16|     797.33|              27|\n",
            "|    0|       0.0|[0.89069950580596...|[2.09790587425231...| 20|    43|             25|            2|            8|     775.83|              17|\n",
            "|    0|       0.0|[0.82964748144149...|[1.58313083648681...| 20|    55|              5|            1|           19|     514.96|              17|\n",
            "+-----+----------+--------------------+--------------------+---+------+---------------+-------------+-------------+-----------+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auc = evaluator.evaluate(predictions)\n",
        "print(f\"AUC: {auc}\")"
      ],
      "metadata": {
        "id": "Ws1sm_F-hrYP",
        "outputId": "4565a815-786c-4342-84fb-785aed075efb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.9773377298583201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Area Under the Curve (AUC) was very very good: 0.97 on the validation set"
      ],
      "metadata": {
        "id": "1EM3GSCJiGRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read test set\n",
        "FILEPATH_test = r\"/content/customer-churn/customer_churn_dataset-testing-master.csv\"\n",
        "df_test = sp.read.options(header=True).csv(FILEPATH)\n",
        "df_test = df_test.drop(\"CustomerID\")"
      ],
      "metadata": {
        "id": "XK6zXWzg-uyQ"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CastRenameCols(Transformer, DefaultParamsWritable, DefaultParamsReadable):\n",
        "    \"\"\"\n",
        "    1) Cast specific columns to IntegerType and Float Type.\n",
        "    2) Drops the column named CustomerID\n",
        "    2) Rename the column replacing space with _\n",
        "\n",
        "    This transformations are specific for the customer churn dataset,\n",
        "    one can change the transformation method to update the class to\n",
        "    their specific needs.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(CastRenameCols, self).__init__()\n",
        "\n",
        "    def _transform(self, df: DataFrame) -> DataFrame:\n",
        "        # Casting columns\n",
        "        df = df.withColumn(\"Age\", col(\"Age\").cast(IntegerType()))\\\n",
        "        .withColumn(\"Tenure\", col(\"Tenure\").cast(IntegerType()))\\\n",
        "        .withColumn(\"Usage Frequency\", col(\"Usage Frequency\").cast(IntegerType()))\\\n",
        "        .withColumn(\"Support Calls\", col(\"Support Calls\").cast(IntegerType()))\\\n",
        "        .withColumn(\"Payment Delay\", col(\"Payment Delay\").cast(IntegerType()))\\\n",
        "        .withColumn(\"Total Spend\", col(\"Total Spend\").cast(FloatType()))\\\n",
        "        .withColumn(\"Last Interaction\", col(\"Last Interaction\").cast(IntegerType()))\\\n",
        "        .withColumn(\"Churn\", col(\"Churn\").cast(IntegerType()))\\\n",
        "        .drop(col(\"CustomerID\"))\n",
        "\n",
        "        # Renaming columns: replace space by _\n",
        "        for column in df.columns:\n",
        "            df = df.withColumnRenamed(column, column if ' ' not in column else column.replace(' ','_'))\n",
        "        return df\n",
        "\n",
        "# Creating object to cast and rename (also drop) specific columns from this dataset\n",
        "cast_rename = CastRenameCols()"
      ],
      "metadata": {
        "id": "haObRT5IgBZX"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline to predict on test set\n",
        "pipelineModelTest = Pipeline(stages=[cast_rename, pipelineModel])"
      ],
      "metadata": {
        "id": "E6b-uBicfkA7"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_test = pipelineModelTest.transform(df_test)"
      ],
      "metadata": {
        "id": "qJruqTYfjaLm",
        "outputId": "fc623689-0345-46d4-e22f-5d36e2171d69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-d015bd9289b2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipelineModelTest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'transform'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_test.select(\"Churn\", \"prediction\", \"probability\", \"rawPrediction\", *prediction_test.columns[:-5]).show()"
      ],
      "metadata": {
        "id": "Gj5pVKAHfIZx",
        "outputId": "9f625d2f-9df2-44a5-9294-ccb7e5d8a59e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PythonException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-441eb5557999>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Churn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"probability\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rawPrediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mprediction_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/content/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/content/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/content/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 273, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/content/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 81, in dump_stream\n    for batch in iterator:\n  File \"/content/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 266, in init_stream_yield_batches\n    for series in iterator:\n  File \"/content/spark-3.1.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 356, in func\n    for result_batch, result_type in result_iter:\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/spark/core.py\", line 1116, in predict_udf\n    raw_preds, class_probs = transform_margin(margins)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\", line 1525, in predict\n    class_probs = super().predict(\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\", line 1114, in predict\n    predts = self.get_booster().inplace_predict(\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 2292, in inplace_predict\n    _check_call(\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 279, in _check_call\n    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\nxgboost.core.XGBoostError: [21:38:46] ../src/predictor/cpu_predictor.cc:381: Check failed: m->NumColumns() == model.learner_model_param->num_feature (0 vs. 7) : Number of columns in data must equal to trained model.\nStack trace:\n  [bt] (0) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x371d43) [0x7daadfd71d43]\n  [bt] (1) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x38c2ba) [0x7daadfd8c2ba]\n  [bt] (2) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x38ce83) [0x7daadfd8ce83]\n  [bt] (3) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x2bd6da) [0x7daadfcbd6da]\n  [bt] (4) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x2ef557) [0x7daadfcef557]\n  [bt] (5) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(+0x1421be) [0x7daadfb421be]\n  [bt] (6) /usr/local/lib/python3.10/dist-packages/xgboost/lib/libxgboost.so(XGBoosterPredictFromDense+0xe7) [0x7daadfb42657]\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7dab072b0e2e]\n  [bt] (8) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7dab072ad493]\n\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "\n",
        "1. run the pipeline on the test set\n",
        "\n",
        "2. continue from page https://learn.microsoft.com/en-us/azure/databricks/_extras/notebooks/source/xgboost-pyspark-new.html saving and reloading the model\n",
        "\n",
        "3. apply categorical features also from the link on step 2 above"
      ],
      "metadata": {
        "id": "RbcGrV3dijej"
      }
    }
  ]
}