{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XQ6EdkxD2SEQ",
        "bdwRgwTy7LKT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up environment\n",
        "\n",
        "\n",
        "1.   Install Java\n",
        "2.   Install and unpack Spark and Hadoop Distributed File System (HDFS)\n",
        "3.   Define environment variables for Java and Spark\n",
        "4.   Install and initialize findspark\n",
        "5.   Check Spark installation\n",
        "\n"
      ],
      "metadata": {
        "id": "XQ6EdkxD2SEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Java installation\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "h3C6AHqF2WTy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading Spark and Hadoop Distributed File System (HDFS) files\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "\n",
        "# Unpacking files\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "TmUYHd7F3IX-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing os library\n",
        "import os\n",
        "\n",
        "# Defining Java's system variable\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# Defining Spark's system variable\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "LMZMRkk73N0t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary to run PySpark\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "rT6OI9tI5fwi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing installed finspark library\n",
        "import findspark\n",
        "\n",
        "# Initialize findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "r3WwHA115pGh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing package to initialize Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# iniciando o spark context\n",
        "sc = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "\n",
        "# Verificando se a sess√£o foi criada\n",
        "sc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fQ2a2f1v5xiJ",
        "outputId": "06325867-b0a2-496f-e6ec-97150f6660ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fb9d9b61540>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://322e0ebabe27:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If everything is right, the output of the last cell should per below:\n",
        "\n",
        "![successful spark instalation.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "u6XsYCJc6fvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading Kaggle Dataset\n",
        "\n",
        "In this case, a customer-churn dataset found [here](https://www.kaggle.com/datasets/muhammadshahidazeem/customer-churn-dataset) will be used. More info on the dataset in this link.\n",
        "\n",
        "1. Install Kaggle package\n",
        "2. Upload kaggle.json file with credentials (follow instructions [here](https://www.kaggle.com/discussions/general/74235))\n",
        "3. Create kaggle directory\n",
        "4. Upload kaggle.json file\n",
        "5. Change permition of kaggle json file\n",
        "6. Copy API command from the Kaggle dataset you want to use by clicking in $\\ \\vdots$  \n",
        " and then \"Copy API Command\". In this case, [this](https://www.kaggle.com/datasets/muhammadshahidazeem/customer-churn-dataset) dataset is being used\n",
        "7. Paste the copied command above in a new code cell and run\n",
        "8. After waiting the download to finish, under the /content diretory you should have the zipped file with the dataset in the /content directory\n",
        "9. Unzip the data into a directory of choice. In this case, \"customer-churn\""
      ],
      "metadata": {
        "id": "bdwRgwTy7LKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Kaggle package\n",
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "Wf8PqZ0u-npT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload your Kaggles API secret KEY in file kaggle.json (link: https://www.kaggle.com/discussions/general/74235)\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "Ba9Js3wLAyaH",
        "outputId": "31cc2db2-63de-4d09-b5d3-9f3020747ce7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d659db1a-75b9-4ecd-a6d0-9bfc77b7f624\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d659db1a-75b9-4ecd-a6d0-9bfc77b7f624\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"igornunespatricio\",\"key\":\"7f5d35c1c5bd7334abed0a8e12172c4c\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create kaggle directory\n",
        "!mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "68kh-oeKBKav"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy kaggle json file to the created directory\n",
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "hgJ1OyaZDPCd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change permission of the file\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "hxb81dm9Dips"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if everything is correct\n",
        "!kaggle datasets list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zLhw1e1DwgS",
        "outputId": "e515c1f0-a33b-4686-df7d-0554ba0d5bfa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                       title                                             size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "--------------------------------------------------------  -----------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "nelgiriyewithana/countries-of-the-world-2023              Global Country Information Dataset 2023           23KB  2023-07-08 20:37:33           9162        356  1.0              \n",
            "juhibhojani/house-price                                   House Price                                        7MB  2023-08-02 16:51:21           1042         38  0.9411765        \n",
            "arnavsmayan/netflix-userbase-dataset                      Netflix Userbase Dataset                          25KB  2023-07-04 07:38:41          10693        190  1.0              \n",
            "alphiree/cardiovascular-diseases-risk-prediction-dataset  Cardiovascular Diseases Risk Prediction Dataset    5MB  2023-07-03 12:12:19           8186        282  1.0              \n",
            "nelgiriyewithana/global-youtube-statistics-2023           Global YouTube Statistics 2023                    60KB  2023-07-28 15:36:38           1772         76  1.0              \n",
            "abhijitdahatonde/zomato-restaurants-dataset               Zomato Restaurants Dataset                       181KB  2023-07-20 13:11:17            939         32  1.0              \n",
            "abhijitdahatonde/fifa-world-cup-all-dataset               FIFA World Cup All Dataset                       192KB  2023-07-31 02:22:58            520         29  1.0              \n",
            "joebeachcapital/ransomware-attacks                        Ransomware Attacks                                34KB  2023-08-03 00:41:08            326         26  1.0              \n",
            "pavankrishnanarne/global-stock-market-2008-present        Global Stock Market (2008-2023)                    1MB  2023-07-30 06:17:44            721         27  0.9411765        \n",
            "alirezajavid1999/google-stock-2010-2023                   Google Stock (2010-2023)                          76KB  2023-07-29 23:57:16            703         29  1.0              \n",
            "sudheerp2147234/salary-dataset-based-on-country-and-race  Salary dataset based on country and race          49KB  2023-07-06 09:10:21           2524         54  1.0              \n",
            "abhijitdahatonde/real-world-smartphones-dataset           Real World Smartphone's Dataset                   17KB  2023-08-02 07:03:01            565         25  1.0              \n",
            "joebeachcapital/worlds-biggest-data-breaches-and-hacks    World's Biggest Data Breaches & Hacks             49KB  2023-08-01 04:31:23            426         34  1.0              \n",
            "joebeachcapital/top-10000-spotify-songs-1960-now          Top 10000 Songs on Spotify 1960-Now                2MB  2023-07-26 00:54:14           1352         54  1.0              \n",
            "mexwell/world-airports                                    ‚úàÔ∏è World Airports                                  5MB  2023-07-25 08:37:38            946         27  1.0              \n",
            "harishkumardatalab/housing-price-prediction               Housing Price Prediction                           5KB  2023-07-07 04:34:24           3390         69  1.0              \n",
            "nathaniellybrand/chicago-car-crash-dataset                Chicago Car Crash Dataset                        116MB  2023-07-31 19:58:33            450         27  1.0              \n",
            "kapturovalexander/bank-credit-scoring                     Bank credit scoring                               65KB  2023-08-08 06:57:40            833         34  1.0              \n",
            "bhanupratapbiswas/olympic-data                            Olympic Data ü•á ‚õ≥ü•ÖüèãÔ∏è‚Äç‚ôÄÔ∏èüö¥‚Äç‚ôÇÔ∏è                         1MB  2023-07-20 09:30:19           1910         57  1.0              \n",
            "khushipitroda/top-250-indian-movies-imdb                  Top 250 Indian movies IMDB                        24KB  2023-08-02 04:37:17            380         30  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the Dataset from Kaggle\n",
        "!kaggle datasets download -d muhammadshahidazeem/customer-churn-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ebi7alJ-sru",
        "outputId": "8ec88376-363e-4519-b4db-0c2ce72b3e85"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading customer-churn-dataset.zip to /content\n",
            "\r  0% 0.00/6.66M [00:00<?, ?B/s]\n",
            "\r100% 6.66M/6.66M [00:00<00:00, 95.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzipping content to another folder\n",
        "!unzip /content/customer-churn-dataset.zip -d customer-churn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ps8yicsWHzmQ",
        "outputId": "b8eb10b8-b558-4dea-e35d-1f99d1ec89cd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/customer-churn-dataset.zip\n",
            "  inflating: customer-churn/customer_churn_dataset-testing-master.csv  \n",
            "  inflating: customer-churn/customer_churn_dataset-training-master.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "UK6kCWK-Jid0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Spark session\n",
        "FILEPATH = \"/content/customer-churn/customer_churn_dataset-training-master.csv\"\n",
        "sp = SparkSession.builder.master(\"local[1]\")\\\n",
        "          .appName(\"Customer-Churn\")\\\n",
        "          .getOrCreate()\n",
        "# Read csv file (if another file  type, search PySpark documentation)\n",
        "df = sp.read.options(header=True).csv(FILEPATH)"
      ],
      "metadata": {
        "id": "bUpG9nAaJlln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show 10 first rows of dataframe\n",
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUnG0vc8Ny7g",
        "outputId": "3c0606bc-0007-41ad-e0e0-1fc4f09fcc6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---+------+------+---------------+-------------+-------------+-----------------+---------------+-----------+----------------+-----+\n",
            "|CustomerID|Age|Gender|Tenure|Usage Frequency|Support Calls|Payment Delay|Subscription Type|Contract Length|Total Spend|Last Interaction|Churn|\n",
            "+----------+---+------+------+---------------+-------------+-------------+-----------------+---------------+-----------+----------------+-----+\n",
            "|         2| 30|Female|    39|             14|            5|           18|         Standard|         Annual|        932|              17|    1|\n",
            "|         3| 65|Female|    49|              1|           10|            8|            Basic|        Monthly|        557|               6|    1|\n",
            "|         4| 55|Female|    14|              4|            6|           18|            Basic|      Quarterly|        185|               3|    1|\n",
            "|         5| 58|  Male|    38|             21|            7|            7|         Standard|        Monthly|        396|              29|    1|\n",
            "|         6| 23|  Male|    32|             20|            5|            8|            Basic|        Monthly|        617|              20|    1|\n",
            "|         8| 51|  Male|    33|             25|            9|           26|          Premium|         Annual|        129|               8|    1|\n",
            "|         9| 58|Female|    49|             12|            3|           16|         Standard|      Quarterly|        821|              24|    1|\n",
            "|        10| 55|Female|    37|              8|            4|           15|          Premium|         Annual|        445|              30|    1|\n",
            "|        11| 39|  Male|    12|              5|            7|            4|         Standard|      Quarterly|        969|              13|    1|\n",
            "|        12| 64|Female|     3|             25|            2|           11|         Standard|      Quarterly|        415|              29|    1|\n",
            "+----------+---+------+------+---------------+-------------+-------------+-----------------+---------------+-----------+----------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(how=\"any\")"
      ],
      "metadata": {
        "id": "juHt2vDgeut7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Showing dataset Schema\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kguq5x4cO9ta",
        "outputId": "c5a1d66b-c410-4684-af84-157164436398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CustomerID: string (nullable = true)\n",
            " |-- Age: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Tenure: string (nullable = true)\n",
            " |-- Usage Frequency: string (nullable = true)\n",
            " |-- Support Calls: string (nullable = true)\n",
            " |-- Payment Delay: string (nullable = true)\n",
            " |-- Subscription Type: string (nullable = true)\n",
            " |-- Contract Length: string (nullable = true)\n",
            " |-- Total Spend: string (nullable = true)\n",
            " |-- Last Interaction: string (nullable = true)\n",
            " |-- Churn: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count rows and columns\n",
        "print(f'Number of rows: {df.count()}')\n",
        "print(f'Number of columns: {len(df.columns)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uPJFaupcxWD",
        "outputId": "16a83274-92c6-4f45-c9d0-3c87bf62070d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 440832\n",
            "Number of columns: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicates by Customer ID\n",
        "df = df.dropDuplicates(['CustomerID'])"
      ],
      "metadata": {
        "id": "KWvGndLGe9hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing column type\n",
        "from pyspark.sql.types import (\n",
        "    StringType, BooleanType, IntegerType, FloatType, DoubleType\n",
        ")\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df = df.withColumn(\"Age\", col(\"Age\").cast(IntegerType()))\\\n",
        ".withColumn(\"Tenure\", col(\"Tenure\").cast(IntegerType()))\\\n",
        ".withColumn(\"Usage Frequency\", col(\"Usage Frequency\").cast(IntegerType()))\\\n",
        ".withColumn(\"Support Calls\", col(\"Support Calls\").cast(IntegerType()))\\\n",
        ".withColumn(\"Payment Delay\", col(\"Payment Delay\").cast(IntegerType()))\\\n",
        ".withColumn(\"Total Spend\", col(\"Total Spend\").cast(FloatType()))\\\n",
        ".withColumn(\"Last Interaction\", col(\"Last Interaction\").cast(IntegerType()))\\\n",
        ".withColumn(\"Churn\", col(\"Churn\").cast(IntegerType()))\\\n",
        ".drop(col(\"CustomerID\"))\n",
        "\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "DUiHvbXKQXXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bcbd400-fa7b-4701-fe8e-c65250e6ddbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Tenure: integer (nullable = true)\n",
            " |-- Usage Frequency: integer (nullable = true)\n",
            " |-- Support Calls: integer (nullable = true)\n",
            " |-- Payment Delay: integer (nullable = true)\n",
            " |-- Subscription Type: string (nullable = true)\n",
            " |-- Contract Length: string (nullable = true)\n",
            " |-- Total Spend: float (nullable = true)\n",
            " |-- Last Interaction: integer (nullable = true)\n",
            " |-- Churn: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing column name to replace space with _\n",
        "for column in df.columns:\n",
        "    df = df.withColumnRenamed(column, column if ' ' not in column else column.replace(' ','_'))\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saJ2XrR7CDPI",
        "outputId": "375fccf6-9316-4fb0-b9b2-7c66946fc031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Tenure: integer (nullable = true)\n",
            " |-- Usage_Frequency: integer (nullable = true)\n",
            " |-- Support_Calls: integer (nullable = true)\n",
            " |-- Payment_Delay: integer (nullable = true)\n",
            " |-- Subscription_Type: string (nullable = true)\n",
            " |-- Contract_Length: string (nullable = true)\n",
            " |-- Total_Spend: float (nullable = true)\n",
            " |-- Last_Interaction: integer (nullable = true)\n",
            " |-- Churn: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "# Number of Churns and not Churns\n",
        "df.groupBy(\"Churn\") \\\n",
        "    .agg(\n",
        "        F.count(\"Churn\").alias(\"num_people\"), \\\n",
        "    )\\\n",
        "    .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taA8MamZ2hKW",
        "outputId": "aa15273e-c0f3-48d9-dd3b-8a64970ac212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+\n",
            "|Churn|num_people|\n",
            "+-----+----------+\n",
            "|    1|    249999|\n",
            "|    0|    190833|\n",
            "+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import struct\n",
        "def groupby_and_describe(df, group_col, stat_col):\n",
        "    \"\"\"\n",
        "    df has to be a pyspark dataframe\n",
        "    Compute statistics for the column in stat_col after grouping\n",
        "    by group_col.\n",
        "    if stat_col is IntegerType or FloatType, calculates quantiles, median, max,...\n",
        "    if stat_col is StringType, count occurence by each category\n",
        "    Take the resulting datafarame and nest the statistics columns.\n",
        "    \"\"\"\n",
        "    assert group_col != stat_col, f\"group_col and stat_col can't be the same, they are {group_col} and {stat_col}, respectively.\"\n",
        "\n",
        "    grouped = df.groupby(group_col)\n",
        "    column = df.schema[stat_col]\n",
        "    # if column is IntegerType or FloatType\n",
        "    if (isinstance(column.dataType, IntegerType) or isinstance(column.dataType, FloatType)) and (column.name != group_col):\n",
        "        # group and create statistics\n",
        "        output = grouped.agg(\n",
        "            F.mean(stat_col).alias(\"mean\"),\n",
        "            F.stddev(stat_col).alias(\"std\"),\n",
        "            F.min(stat_col).alias(\"min\"),\n",
        "            F.percentile_approx(stat_col, 0.25).alias(\"25%\"),\n",
        "            F.percentile_approx(stat_col, 0.50).alias(\"50%\"),\n",
        "            F.percentile_approx(stat_col, 0.75).alias(\"75%\"),\n",
        "            F.max(stat_col).alias(\"max\"),\n",
        "        )\n",
        "        # structure the dataframe\n",
        "        output = output.select(\n",
        "            group_col,\n",
        "            struct(\"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\")\\\n",
        "            .alias(stat_col)\n",
        "        )\n",
        "        return output\n",
        "    # if the column is StringType\n",
        "    elif isinstance(column.dataType, StringType) and (column.name != group_col):\n",
        "        # group by group_col and pivot the stat_col and count occurences\n",
        "        output = df.groupby(group_col).pivot(stat_col).count()\n",
        "        # change columns from long to IntegerType\n",
        "        cols2change = [col for col in output.columns if col != group_col]\n",
        "        for column in cols2change:\n",
        "            output = output.withColumn(column, col(column).cast(IntegerType()))\n",
        "        # Structure the dataframe\n",
        "        output = output.select(\n",
        "            group_col,\n",
        "            struct(cols2change)\\\n",
        "            .alias(stat_col)\n",
        "        )\n",
        "        return output\n",
        "\n",
        "# Join statistics for each column by Churn value keeping nested columns\n",
        "joined = None\n",
        "aggregate_cols = [col for col in df.columns if col != \"Churn\"]\n",
        "for column in aggregate_cols:\n",
        "    print(f'Variable: {column}')\n",
        "    grouped_temp = groupby_and_describe(\n",
        "        df=df,\n",
        "        group_col=\"Churn\",\n",
        "        stat_col=column\n",
        "        )\n",
        "    if not joined:\n",
        "        joined = grouped_temp\n",
        "    else:\n",
        "        joined = joined.join(grouped_temp, on=[\"Churn\"])\n",
        "\n",
        "joined.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYotM71cqxD-",
        "outputId": "7842b8c9-ced3-4a24-f15c-4263a76fe897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variable: Age\n",
            "Variable: Gender\n",
            "Variable: Tenure\n",
            "Variable: Usage_Frequency\n",
            "Variable: Support_Calls\n",
            "Variable: Payment_Delay\n",
            "Variable: Subscription_Type\n",
            "Variable: Contract_Length\n",
            "Variable: Total_Spend\n",
            "Variable: Last_Interaction\n",
            "root\n",
            " |-- Churn: integer (nullable = true)\n",
            " |-- Age: struct (nullable = false)\n",
            " |    |-- mean: double (nullable = true)\n",
            " |    |-- std: double (nullable = true)\n",
            " |    |-- min: integer (nullable = true)\n",
            " |    |-- 25%: integer (nullable = true)\n",
            " |    |-- 50%: integer (nullable = true)\n",
            " |    |-- 75%: integer (nullable = true)\n",
            " |    |-- max: integer (nullable = true)\n",
            " |-- Gender: struct (nullable = false)\n",
            " |    |-- Female: integer (nullable = true)\n",
            " |    |-- Male: integer (nullable = true)\n",
            " |-- Tenure: struct (nullable = false)\n",
            " |    |-- mean: double (nullable = true)\n",
            " |    |-- std: double (nullable = true)\n",
            " |    |-- min: integer (nullable = true)\n",
            " |    |-- 25%: integer (nullable = true)\n",
            " |    |-- 50%: integer (nullable = true)\n",
            " |    |-- 75%: integer (nullable = true)\n",
            " |    |-- max: integer (nullable = true)\n",
            " |-- Usage_Frequency: struct (nullable = false)\n",
            " |    |-- mean: double (nullable = true)\n",
            " |    |-- std: double (nullable = true)\n",
            " |    |-- min: integer (nullable = true)\n",
            " |    |-- 25%: integer (nullable = true)\n",
            " |    |-- 50%: integer (nullable = true)\n",
            " |    |-- 75%: integer (nullable = true)\n",
            " |    |-- max: integer (nullable = true)\n",
            " |-- Support_Calls: struct (nullable = false)\n",
            " |    |-- mean: double (nullable = true)\n",
            " |    |-- std: double (nullable = true)\n",
            " |    |-- min: integer (nullable = true)\n",
            " |    |-- 25%: integer (nullable = true)\n",
            " |    |-- 50%: integer (nullable = true)\n",
            " |    |-- 75%: integer (nullable = true)\n",
            " |    |-- max: integer (nullable = true)\n",
            " |-- Payment_Delay: struct (nullable = false)\n",
            " |    |-- mean: double (nullable = true)\n",
            " |    |-- std: double (nullable = true)\n",
            " |    |-- min: integer (nullable = true)\n",
            " |    |-- 25%: integer (nullable = true)\n",
            " |    |-- 50%: integer (nullable = true)\n",
            " |    |-- 75%: integer (nullable = true)\n",
            " |    |-- max: integer (nullable = true)\n",
            " |-- Subscription_Type: struct (nullable = false)\n",
            " |    |-- Basic: integer (nullable = true)\n",
            " |    |-- Premium: integer (nullable = true)\n",
            " |    |-- Standard: integer (nullable = true)\n",
            " |-- Contract_Length: struct (nullable = false)\n",
            " |    |-- Annual: integer (nullable = true)\n",
            " |    |-- Monthly: integer (nullable = true)\n",
            " |    |-- Quarterly: integer (nullable = true)\n",
            " |-- Total_Spend: struct (nullable = false)\n",
            " |    |-- mean: double (nullable = true)\n",
            " |    |-- std: double (nullable = true)\n",
            " |    |-- min: float (nullable = true)\n",
            " |    |-- 25%: float (nullable = true)\n",
            " |    |-- 50%: float (nullable = true)\n",
            " |    |-- 75%: float (nullable = true)\n",
            " |    |-- max: float (nullable = true)\n",
            " |-- Last_Interaction: struct (nullable = false)\n",
            " |    |-- mean: double (nullable = true)\n",
            " |    |-- std: double (nullable = true)\n",
            " |    |-- min: integer (nullable = true)\n",
            " |    |-- 25%: integer (nullable = true)\n",
            " |    |-- 50%: integer (nullable = true)\n",
            " |    |-- 75%: integer (nullable = true)\n",
            " |    |-- max: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "boxplots = {}\n",
        "\n",
        "for column in df.schema:\n",
        "    print(column.name)\n",
        "    temp = df.select(\"Churn\", column.name)\n",
        "    if (isinstance(column.dataType, IntegerType) or isinstance(column.dataType, FloatType)) and (column.name != \"Churn\"):\n",
        "        fig = px.box(\n",
        "            temp.toPandas(),\n",
        "            x=\"Churn\", y=column.name, color=\"Churn\",\n",
        "            title=f\"{column.name} x Churn\"\n",
        "        )\\\n",
        "        .update_layout(\n",
        "            yaxis_title=None, xaxis_title=None,\n",
        "            title_x=0.5, title_font_size=30\n",
        "        )\n",
        "        boxplots[column.name] = fig\n",
        "    elif (isinstance(column.dataType, StringType) and (column.name != \"Churn\")):\n",
        "        # create bar chart with plotly\n",
        "        grouped_test = df.select(\"Churn\", column.name).groupby(\"Churn\").pivot(column.name).count().withColumn(\"Churn\", F.col(\"Churn\").cast(\"string\"))\n",
        "        y_axis = [item for item in grouped_test.columns if item != \"Churn\"]\n",
        "        fig = px.histogram(\n",
        "            grouped_test.toPandas(),\n",
        "            x=\"Churn\",\n",
        "            y=y_axis,\n",
        "            barnorm=\"percent\",\n",
        "            text_auto='.2f'\n",
        "        ).update_layout(\n",
        "            yaxis_title=None, xaxis_title=None, title=f\"Churn x {column.name} (%)\",\n",
        "            title_x=0.5, title_font_size=30\n",
        "        )\n",
        "        boxplots[column.name] = fig\n",
        "\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "if not os.path.isdir(\"charts\"):\n",
        "    os.mkdir(\"charts\")\n",
        "\n",
        "for key, chart in boxplots.items():\n",
        "    chart.write_html(f\"charts/boxplot_{key}_x_Churn.html\")\n",
        "\n",
        "# for downloading files one by one\n",
        "# [files.download(\"charts/\" + item) for item in os.listdir(\"charts\")]\n",
        "\n",
        "# for creating a zip file with the charts and downloading it\n",
        "!zip -r charts.zip charts/\n",
        "files.download('charts.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "3k3mNoloi8Fq",
        "outputId": "1d7fafb6-2d70-4fe5-f198-88480eda599e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Age\n",
            "Gender\n",
            "Tenure\n",
            "Usage_Frequency\n",
            "Support_Calls\n",
            "Payment_Delay\n",
            "Subscription_Type\n",
            "Contract_Length\n",
            "Total_Spend\n",
            "Last_Interaction\n",
            "Churn\n",
            "  adding: charts/ (stored 0%)\n",
            "  adding: charts/boxplot_Gender_x_Churn.html (deflated 70%)\n",
            "  adding: charts/boxplot_Contract_Length_x_Churn.html (deflated 70%)\n",
            "  adding: charts/boxplot_Payment_Delay_x_Churn.html (deflated 74%)\n",
            "  adding: charts/boxplot_Total_Spend_x_Churn.html (deflated 76%)\n",
            "  adding: charts/boxplot_Tenure_x_Churn.html (deflated 74%)\n",
            "  adding: charts/boxplot_Subscription_Type_x_Churn.html (deflated 70%)\n",
            "  adding: charts/boxplot_Usage_Frequency_x_Churn.html (deflated 74%)\n",
            "  adding: charts/boxplot_Last_Interaction_x_Churn.html (deflated 74%)\n",
            "  adding: charts/boxplot_Support_Calls_x_Churn.html (deflated 76%)\n",
            "  adding: charts/boxplot_Age_x_Churn.html (deflated 74%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8f7242fb-c8f9-4d7c-8705-b0a302c381d0\", \"charts.zip\", 14272425)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning"
      ],
      "metadata": {
        "id": "ei4QRDE23Wx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sp = SparkSession.builder.master(\"local[1]\")\\\n",
        "          .appName(\"Customer-Churn\")\\\n",
        "          .getOrCreate()\n",
        "\n",
        "FILEPATH = \"/content/customer-churn/customer_churn_dataset-training-master.csv\"\n",
        "# Read csv file (if another file  type, search PySpark documentation)\n",
        "df = sp.read.options(header=True).csv(FILEPATH)\n",
        "\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "NHEV5rI13z62",
        "outputId": "25c74e76-87ed-4d65-d4d7-bbbb77acfc45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CustomerID: string (nullable = true)\n",
            " |-- Age: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Tenure: string (nullable = true)\n",
            " |-- Usage Frequency: string (nullable = true)\n",
            " |-- Support Calls: string (nullable = true)\n",
            " |-- Payment Delay: string (nullable = true)\n",
            " |-- Subscription Type: string (nullable = true)\n",
            " |-- Contract Length: string (nullable = true)\n",
            " |-- Total Spend: string (nullable = true)\n",
            " |-- Last Interaction: string (nullable = true)\n",
            " |-- Churn: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_churns = df.rdd.map(lambda x: x[\"Churn\"]=='1').sum()\n",
        "total = df.count()\n",
        "print(f'Churns: {num_churns}\\nTotal Churns: {total}\\nProportion of Churns: {num_churns/total:.4%}')"
      ],
      "metadata": {
        "id": "086zpZk_73fn",
        "outputId": "5c9e75b8-a1c1-4ebe-d681-52966e850246",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Churns: 249999\n",
            "Total Churns: 440833\n",
            "Proportion of Churns: 56.7106%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A \"model\" predicting only churns would predict 56.71% correct answers (which in this case would also be the accuracy)."
      ],
      "metadata": {
        "id": "ddiJpK8pIOow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers to apply at pipeline"
      ],
      "metadata": {
        "id": "P_doM_Z4ybkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import (\n",
        "    StringType, BooleanType, IntegerType, FloatType, DoubleType\n",
        ")\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "from pyspark.ml import Transformer\n",
        "from pyspark import ml\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark import keyword_only\n",
        "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
        "from pyspark.ml.param import Param, Params, TypeConverters\n",
        "\n",
        "class DropNA(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
        "    \"\"\"\n",
        "    Transormer to drop nan rows.\n",
        "    \"\"\"\n",
        "\n",
        "    how = Param(Params._dummy(), 'how', \"any or all\", TypeConverters.toString)\n",
        "\n",
        "    @keyword_only\n",
        "    def __init__(self, how: str = \"any\"):\n",
        "        super(DropNA, self).__init__()\n",
        "        self._setDefault(how=\"any\")\n",
        "        kwargs = self._input_kwargs\n",
        "        self.setParams(**kwargs)\n",
        "\n",
        "    @keyword_only\n",
        "    def setParams(self, outputCols=None, how=\"any\"):\n",
        "        \"\"\"\n",
        "        Sets params for this transformer\n",
        "        \"\"\"\n",
        "        kwargs = self._input_kwargs\n",
        "        return self._set(**kwargs)\n",
        "\n",
        "    def setHow(self, how):\n",
        "        \"\"\"\n",
        "        Sets the value of :py:attr:`how`.\n",
        "        \"\"\"\n",
        "        return self._set(how=how)\n",
        "\n",
        "    def getHow(self):\n",
        "        \"\"\"\n",
        "        Gets the value of :py:attr:`how` or its default value.\n",
        "        \"\"\"\n",
        "        return self.getOrDefault(self.how)\n",
        "\n",
        "    def _transform(self, df):\n",
        "        df = df.dropna(how=self.getHow())\n",
        "        return df\n",
        "\n",
        "class DropDuplicates(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
        "    \"\"\"\n",
        "    Transformer to drop duplicates by columns.\n",
        "    \"\"\"\n",
        "\n",
        "    columns = Param(Params._dummy(), 'columns', 'Columns to seek for duplicates', TypeConverters.toListString)\n",
        "\n",
        "    @keyword_only\n",
        "    def __init__(self, columns: list[str]= None):\n",
        "        super(DropDuplicates, self).__init__()\n",
        "        self._setDefault(columns=None)\n",
        "        kwargs = self._input_kwargs\n",
        "        self.setParams(**kwargs)\n",
        "\n",
        "    @keyword_only\n",
        "    def setParams(self, outputCols=None, columns=None):\n",
        "        \"\"\"\n",
        "        Sets params for this transformer\n",
        "        \"\"\"\n",
        "        kwargs = self._input_kwargs\n",
        "        return self._set(**kwargs)\n",
        "\n",
        "    def setColumns(self, columns):\n",
        "        \"\"\"\n",
        "        Sets the value of :py:attr:`columns`.\n",
        "        \"\"\"\n",
        "        return self._set(columns=columns)\n",
        "\n",
        "    def getColumns(self):\n",
        "        \"\"\"\n",
        "        Gets the value of :py:attr:`columns` or its default value.\n",
        "        \"\"\"\n",
        "        return self.getOrDefault(self.columns)\n",
        "\n",
        "    def _transform(self, df: DataFrame) -> DataFrame:\n",
        "        df = df.dropDuplicates(self.getColumns())\n",
        "        return df\n",
        "\n",
        "class DropCols(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
        "    \"\"\"\n",
        "    Transformer to drop specified columns in parameter columns from dataframe.\n",
        "    \"\"\"\n",
        "\n",
        "    columns = Param(Params._dummy(), 'columns', 'Columns to drop', TypeConverters.toListString)\n",
        "\n",
        "    @keyword_only\n",
        "    def __init__(self, columns: list[str]=None):\n",
        "        super(DropCols, self).__init__()\n",
        "        self._setDefault(columns=None)\n",
        "        kwargs = self._input_kwargs\n",
        "        self.setParams(**kwargs)\n",
        "\n",
        "    @keyword_only\n",
        "    def setParams(self, columns=None):\n",
        "        \"\"\"\n",
        "        Set params for this transformer\n",
        "        \"\"\"\n",
        "        kwargs = self._input_kwargs\n",
        "        return self._set(**kwargs)\n",
        "\n",
        "    def setColumns(self, columns):\n",
        "        \"\"\"\n",
        "        Sets the value of :py:attr:`columns`\n",
        "        \"\"\"\n",
        "        return self._set(columns=columns)\n",
        "\n",
        "    def getColumns(self):\n",
        "        \"\"\"\n",
        "        Gets the value of :py:attr:`columns` or its default value.\n",
        "        \"\"\"\n",
        "        return self.getOrDefault(self.columns)\n",
        "\n",
        "    def _transform(self, df: DataFrame) -> DataFrame:\n",
        "        df = df.drop(*self.getColumns())\n",
        "        return df\n",
        "\n",
        "class KeepIntFloatCols(Transformer, DefaultParamsWritable, DefaultParamsReadable):\n",
        "    \"\"\"\n",
        "    Transformer class that only keeps IntegerType and FloatType columns.\n",
        "    Also maintains the label column which the user should specify in the parameter\n",
        "    called labelCol.\n",
        "    \"\"\"\n",
        "    def __init__(self, labelCol: str = \"Churn\"):\n",
        "        super(KeepIntFloatCols, self).__init__()\n",
        "        self.labelCol = labelCol\n",
        "\n",
        "    def _transform(self, df: DataFrame) -> DataFrame:\n",
        "        cols = [column.name for column in df.schema if (isinstance(column.dataType, IntegerType) or isinstance(column.dataType, FloatType) or column.name == self.labelCol)]\n",
        "        return df.select(cols)\n",
        "\n",
        "class CustomVectorAssembler(Transformer, DefaultParamsWritable, DefaultParamsReadable):\n",
        "    \"\"\"\n",
        "    The only difference from VectorAssembler class is that:\n",
        "    1) Removes the label column (labelCol parameter) from the assembler\n",
        "    2) Set all columns from df as InputCols (if as a stage in pipeline, uses the previous df)\n",
        "    \"\"\"\n",
        "\n",
        "    labelCol = Param(Params._dummy(), 'labelCol', 'Label column', TypeConverters.toString)\n",
        "\n",
        "    @keyword_only\n",
        "    def __init__(self, labelCol: str = None):\n",
        "        super(CustomVectorAssembler, self).__init__()\n",
        "        self.transformer = VectorAssembler(handleInvalid=\"skip\", outputCol='features')\n",
        "        kwargs = self._input_kwargs\n",
        "        self.setParams(**kwargs)\n",
        "\n",
        "    @keyword_only\n",
        "    def setParams(self, labelCol: str = None):\n",
        "        kwargs = self._input_kwargs\n",
        "        return self._set(**kwargs)\n",
        "\n",
        "    def setlabelCol(self, labelCol: str = None):\n",
        "        return self._set(labelCol=labelCol)\n",
        "\n",
        "    def getlabelCol(self):\n",
        "        return self.getOrDefault(self.labelCol)\n",
        "\n",
        "    def _transform(self, df) -> DataFrame:\n",
        "        inputCols = [col for col in df.columns if col != self.getlabelCol()]\n",
        "        self.transformer.setInputCols(inputCols)\n",
        "        df = self.transformer.transform(df)\n",
        "        df = df.drop(*inputCols)\n",
        "        return df\n",
        "\n",
        "class CastCols(Transformer, DefaultParamsWritable, DefaultParamsReadable):\n",
        "    \"\"\"\n",
        "    Cast specific columns to IntegerType and Float Type.\n",
        "    This transformations is specific for the customer churn dataset,\n",
        "    one can change the transformation method to update the class to\n",
        "    their specific needs.\n",
        "    \"\"\"\n",
        "\n",
        "    columns = Param(Params._dummy(), 'columns', 'dictionary of form {column_name, type} to cast columns')\n",
        "\n",
        "    @keyword_only\n",
        "    def __init__(self, columns=dict):\n",
        "        super(CastCols, self).__init__()\n",
        "        self._setDefault(columns=None)\n",
        "        kwargs = self._input_kwargs\n",
        "        self.setParams(**kwargs)\n",
        "\n",
        "    @keyword_only\n",
        "    def setParams(self, columns=None):\n",
        "        \"\"\"\n",
        "        Set params for this transformer\n",
        "        \"\"\"\n",
        "        kwargs = self._input_kwargs\n",
        "        return self._set(**kwargs)\n",
        "\n",
        "    def setColumns(self, columns):\n",
        "        \"\"\"\n",
        "        Sets the value of :py:attr:`columns`\n",
        "        \"\"\"\n",
        "        return self._set(columns=columns)\n",
        "\n",
        "    def getColumns(self):\n",
        "        \"\"\"\n",
        "        Gets the value of :py:attr:`columns` or its default value.\n",
        "        \"\"\"\n",
        "        return self.getOrDefault(self.columns)\n",
        "\n",
        "    def _transform(self, df: DataFrame) -> DataFrame:\n",
        "        # Casting columns\n",
        "        for column, col_type in self.getColumns().items():\n",
        "            if col_type == \"integer\":\n",
        "                df = df.withColumn(column, col(column).cast(IntegerType()))\n",
        "            elif col_type == \"float\":\n",
        "                df = df.withColumn(column, col(column).cast(FloatType()))\n",
        "            elif col_type == 'string':\n",
        "                df = df.withColumn(column, col(column).cast(StringType()))\n",
        "\n",
        "        # df = df.withColumn(\"Age\", col(\"Age\").cast(IntegerType()))\\\n",
        "        # .withColumn(\"Tenure\", col(\"Tenure\").cast(IntegerType()))\\\n",
        "        # .withColumn(\"Usage Frequency\", col(\"Usage Frequency\").cast(IntegerType()))\\\n",
        "        # .withColumn(\"Support Calls\", col(\"Support Calls\").cast(IntegerType()))\\\n",
        "        # .withColumn(\"Payment Delay\", col(\"Payment Delay\").cast(IntegerType()))\\\n",
        "        # .withColumn(\"Total Spend\", col(\"Total Spend\").cast(FloatType()))\\\n",
        "        # .withColumn(\"Last Interaction\", col(\"Last Interaction\").cast(IntegerType()))\\\n",
        "        # .withColumn(\"Churn\", col(\"Churn\").cast(IntegerType()))\n",
        "        return df\n",
        "\n",
        "class RenameCols(Transformer, DefaultParamsWritable, DefaultParamsReadable):\n",
        "    \"\"\"\n",
        "    Rename the column replacing space with replacer parameter\n",
        "    This transformations are specific for the customer churn dataset,\n",
        "    one can change the transformation method to update the class to\n",
        "    their specific needs.\n",
        "    \"\"\"\n",
        "\n",
        "    to_replace = Param(Params._dummy(), 'to_replace', 'String to replace', TypeConverters.toString)\n",
        "    replacer = Param(Params._dummy(), 'replacer', 'String to replace to', TypeConverters.toString)\n",
        "\n",
        "    @keyword_only\n",
        "    def __init__(self, to_replace: str = \" \", replacer: str = \"_\"):\n",
        "        super(RenameCols, self).__init__()\n",
        "        self._setDefault(to_replace=\" \")\n",
        "        self._setDefault(replacer=\"_\")\n",
        "\n",
        "    @keyword_only\n",
        "    def setParams(self, to_replace=\" \", replacer=\"_\"):\n",
        "        \"\"\"\n",
        "        Set params for this transformer\n",
        "        \"\"\"\n",
        "        kwargs = self._input_kwargs\n",
        "        return self._set(**kwargs)\n",
        "\n",
        "    def setReplacer(self, replacer):\n",
        "        \"\"\"\n",
        "        Sets the value of :py:attr:`replacer`\n",
        "        \"\"\"\n",
        "        return self._set(replacer=replacer)\n",
        "\n",
        "    def getReplacer(self):\n",
        "        \"\"\"\n",
        "        Gets the value of :py:attr:`replacer` or its default value.\n",
        "        \"\"\"\n",
        "        return self.getOrDefault(self.replacer)\n",
        "\n",
        "    def setToReplace(self, to_replace):\n",
        "        \"\"\"\n",
        "        Sets the value of :py:attr:`to_replace`\n",
        "        \"\"\"\n",
        "        return self._set(to_replace=to_replace)\n",
        "\n",
        "    def getToReplace(self):\n",
        "        \"\"\"\n",
        "        Gets the value of :py:attr:`to_replace` or its default value.\n",
        "        \"\"\"\n",
        "        return self.getOrDefault(self.to_replace)\n",
        "\n",
        "    def _transform(self, df: DataFrame) -> DataFrame:\n",
        "        # Renaming columns\n",
        "        for column in df.columns:\n",
        "            df = df.withColumnRenamed(column, column if self.getToReplace() not in column else column.replace(self.getToReplace(), self.getReplacer()))\n",
        "        return df"
      ],
      "metadata": {
        "id": "D7wK0NdryZfc"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import abc\n",
        "import sys, inspect\n",
        "\n",
        "def set_module(clazz: abc.ABCMeta):\n",
        "    \"\"\"\n",
        "    Adds the clazz to the module.\n",
        "    Further, this will help when loading pipeline in pyspark.\n",
        "    Solution in: https://www.reddit.com/r/MachineLearning/comments/dohht7/p_spark_ml_saving_pyspark_custom_transformers_in/?rdt=38667\n",
        "    \"\"\"\n",
        "    m = __import__(clazz.__module__)\n",
        "    setattr(m, clazz.__name__, clazz)\n",
        "\n",
        "for name, obj in inspect.getmembers(sys.modules[__name__], inspect.isclass):\n",
        "    if inspect.getmodule(obj).__name__.startswith(\"__main__\"):\n",
        "        set_module(obj)"
      ],
      "metadata": {
        "id": "ZKTF9g0L4E8O"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1:\n",
        "\n",
        "- Base Model\n",
        "- Not using StringType columns\n",
        "- No Feature Engineering\n",
        "- No normalization\n",
        "- Using only IntegerType and FloatType columns\n",
        "- Nothing else was done"
      ],
      "metadata": {
        "id": "sk_qv11oKINo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating transformer object for dropping rows with at least one NAN value\n",
        "drop_na = DropNA(how=\"any\")\n",
        "\n",
        "# Creating transformer object for dropping duplicates based on CustomerID column\n",
        "drop_duplicates = DropDuplicates(columns=['CustomerID'])\n",
        "\n",
        "# Creating transformer object for dropping unnecessary columns\n",
        "drop_cols = DropCols(columns=[\"CustomerID\"])\n",
        "\n",
        "# Creating transformer object to cast columns\n",
        "cast_cols = CastCols(\n",
        "    columns={\n",
        "        \"Age\": \"integer\",\n",
        "        \"Tenure\": \"integer\",\n",
        "        \"Usage Frequency\": \"integer\",\n",
        "        \"Support Calls\": \"integer\",\n",
        "        \"Payment Delay\": \"integer\",\n",
        "        \"Total Spend\": \"float\",\n",
        "        \"Last Interaction\": \"integer\",\n",
        "        \"Churn\": \"integer\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Creating transformer object to rename columns\n",
        "rename_cols = RenameCols(to_replace=\" \", replacer=\"_\")\n",
        "\n",
        "# Creating tranformer object for keeping only IntegerType and FloatType columns (include also labelCol)\n",
        "keep_int_float_cols = KeepIntFloatCols(labelCol=\"Churn\")\n",
        "\n",
        "# Creating transformer object for vector assemble (with input cols from the previous stage/df in pipeline)\n",
        "custom_vector_assembler = CustomVectorAssembler(labelCol='Churn')"
      ],
      "metadata": {
        "id": "4LsrcLgH3oyF"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_na = drop_na.transform(df)\n",
        "no_dupli = drop_duplicates.transform(no_na)\n",
        "dropped_cols = drop_cols.transform(no_dupli)\n",
        "casted_cols = cast_cols.transform(dropped_cols)\n",
        "renamed_cols = rename_cols.transform(casted_cols)\n",
        "num_cols = keep_int_float_cols.transform(renamed_cols)\n",
        "num_cols.show()\n",
        "assembler = custom_vector_assembler.transform(num_cols)\n",
        "assembler.select('features').first()"
      ],
      "metadata": {
        "id": "WpYDC3PR8R57",
        "outputId": "b748e8b6-41c3-481e-bbac-30690c8fb32e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---------------+-------------+-------------+-----------+----------------+-----+\n",
            "|Age|Tenure|Usage_Frequency|Support_Calls|Payment_Delay|Total_Spend|Last_Interaction|Churn|\n",
            "+---+------+---------------+-------------+-------------+-----------+----------------+-----+\n",
            "| 42|    25|              9|            5|           22|      810.0|               2|    1|\n",
            "| 52|    40|              2|            5|           16|      460.0|              20|    1|\n",
            "| 42|    44|              8|            6|            7|      324.0|              23|    1|\n",
            "| 41|     6|             11|            2|            7|      328.0|              10|    1|\n",
            "| 43|    52|             27|            1|           12|      559.0|              27|    1|\n",
            "| 32|    37|              8|            3|           23|      123.0|              14|    1|\n",
            "| 53|    36|              3|            5|           21|      472.0|              14|    1|\n",
            "| 39|    52|             22|            8|           23|      613.0|              28|    1|\n",
            "| 29|    15|              7|            3|            4|      830.0|              27|    1|\n",
            "| 45|    43|             18|            8|           24|      413.0|              23|    1|\n",
            "| 37|    49|              1|            6|            6|      197.0|              26|    1|\n",
            "| 64|    19|              4|            9|           18|      328.0|              10|    1|\n",
            "| 65|    52|             17|            5|           22|      229.0|              15|    1|\n",
            "| 64|    55|              5|            7|            7|      391.0|              14|    1|\n",
            "| 38|     5|             24|            2|           19|      614.0|              27|    1|\n",
            "| 41|     9|             22|            7|            7|      834.0|              19|    1|\n",
            "| 36|    59|             28|            0|           15|      423.0|              17|    1|\n",
            "| 61|     6|             17|            1|           12|      577.0|              20|    1|\n",
            "| 35|    15|              4|            0|           18|      512.0|              20|    1|\n",
            "| 36|    15|             25|            7|           19|      712.0|              26|    1|\n",
            "+---+------+---------------+-------------+-------------+-----------+----------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(features=DenseVector([42.0, 25.0, 9.0, 5.0, 22.0, 810.0, 2.0]))"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Pipeline(stages=[\n",
        "        drop_na, drop_duplicates, drop_cols, cast_cols, rename_cols,\n",
        "        keep_int_float_cols, custom_vector_assembler\n",
        "        ]\n",
        ").fit(df).transform(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mG69XXw5G46t",
        "outputId": "65412346-d3eb-44dc-bcb8-d5b488a9ddc6"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Churn: int, features: vector]"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost.spark.estimator import SparkXGBClassifier\n",
        "\n",
        "# Creating the XGBoost Classifier: input col is 'features'by default\n",
        "gbt_classifier = GBTClassifier(labelCol=\"Churn\", featuresCol=\"features\", maxIter=10)\n",
        "# xgb_classifier = SparkXGBClassifier(label_col='Churn', features_col=\"features\", num_workers=2)\n",
        "\n",
        "# Create pipeline of model 1\n",
        "pipeline_model_1 = Pipeline(\n",
        "    stages=[\n",
        "        drop_na, drop_duplicates, drop_cols, cast_cols, rename_cols,\n",
        "        keep_int_float_cols, custom_vector_assembler,\n",
        "        gbt_classifier\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Object with grid of parameters to try\n",
        "paramGrid = ParamGridBuilder()\\\n",
        "  .addGrid(gbt_classifier.maxDepth, [2, 5, 10, 20])\\\n",
        "  .build()\n",
        "\n",
        "# Define object for evaluation metric\n",
        "evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=gbt_classifier.getLabelCol(),\n",
        "    rawPredictionCol=gbt_classifier.getPredictionCol()\n",
        ")\n",
        "\n",
        "# Define object for crossvalidation (model tuning).\n",
        "cross_val_model_1 = CrossValidator(\n",
        "    estimator=pipeline_model_1,\n",
        "    evaluator=evaluator,\n",
        "    estimatorParamMaps=paramGrid\n",
        ")"
      ],
      "metadata": {
        "id": "IHC0QLGC95QS"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FOR TESTING PURPOSES, LIMITING THE TRAINING DATA\n",
        "df_train = df.limit(10)"
      ],
      "metadata": {
        "id": "xZZ-NewxuvU_"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "best_model_1 = cross_val_model_1.fit(df_train)"
      ],
      "metadata": {
        "id": "SL9r0hR7ZSkK"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get best model from cross validation object\n",
        "best_model = best_model_1.bestModel"
      ],
      "metadata": {
        "id": "5kYAyz7Gj5Lt"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "predictions = best_model.transform(df)\n",
        "\n",
        "predictions.groupby(\"Churn\", \"prediction\").count().show()"
      ],
      "metadata": {
        "id": "jDKVFCR07nyB",
        "outputId": "de70a58c-87ad-46e1-f9ae-e16ea879f06c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+\n",
            "|Churn|prediction| count|\n",
            "+-----+----------+------+\n",
            "|    1|       1.0|249999|\n",
            "|    0|       1.0|190833|\n",
            "+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.select('probability').distinct().collect()"
      ],
      "metadata": {
        "id": "AtX5EtfDFWel",
        "outputId": "e7373d30-efe9-4c95-f9a6-838fba15870b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(probability=DenseVector([0.0659, 0.9341]))]"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluaton of the trained model on the train dataset\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f\"AUC: {auc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qTTYuTX_6AF",
        "outputId": "b809ec69-10bd-4fdd-858a-33c7fe81c035"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read test set\n",
        "FILEPATH_test = r\"/content/customer-churn/customer_churn_dataset-testing-master.csv\"\n",
        "df_test = sp.read.options(header=True).csv(FILEPATH)\n",
        "\n",
        "# Predictions on test set\n",
        "predictions = best_model.transform(df_test)"
      ],
      "metadata": {
        "id": "i7FEOwSVgWb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.select('Churn', 'prediction').groupby('Churn').pivot('prediction').count().show()"
      ],
      "metadata": {
        "id": "OXaxfDctg8cE",
        "outputId": "3bef708f-4f99-4b4a-e885-02fbd33ae8a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+\n",
            "|Churn|   1.0|\n",
            "+-----+------+\n",
            "|    1|249999|\n",
            "|    0|190833|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluaton of the trained model on the test dataset\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f\"AUC: {auc}\")"
      ],
      "metadata": {
        "id": "Ws1sm_F-hrYP",
        "outputId": "61776985-43f3-4d0a-9746-1d800dc09cf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Area Under the Curve (AUC) was very very good: 0.9768 on the validation set"
      ],
      "metadata": {
        "id": "1EM3GSCJiGRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "best_model.write().overwrite().save(\"best_model\")"
      ],
      "metadata": {
        "id": "Or25oVoNNeWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import CrossValidatorModel\n",
        "\n",
        "# Loading the model\n",
        "best_model_loaded = PipelineModel.load(r\"best_model/\")"
      ],
      "metadata": {
        "id": "cM6u9jVZW2nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_test_loaded_model = best_model_loaded.transform(df_test)\n",
        "auc_test_loaded_model = evaluator.evaluate(predictions_test_loaded_model)\n",
        "\n",
        "print(f\"AUC: {auc_test_loaded_model}\")"
      ],
      "metadata": {
        "id": "nKJmOMX6fpsK",
        "outputId": "af688130-6f44-428f-a8c3-a2dcb50d036e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "\n",
        "1. Need to check how to save and load transformers with parameters passed by user in pyspark\n",
        "\n",
        "2. apply categorical features also from the link on step 2 above"
      ],
      "metadata": {
        "id": "RbcGrV3dijej"
      }
    }
  ]
}